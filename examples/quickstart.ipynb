{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick start\n",
    "\n",
    "This is an introduction to neural network-based survival modeling using PySaRe.\n",
    "\n",
    "PySaRe models essentially extend the functionality of the PyTorch Module with functionality for survival analysis. Most importantly, they provide the\n",
    "\n",
    "- Survival function\n",
    "- Density function\n",
    "- Hazard function\n",
    "- log-likelihood\n",
    "\n",
    "In this example it is shown how to:\n",
    "\n",
    "- [Create a survival dataset ](#creating-survival-datasets)\n",
    "- [Define a model](#defining-models)\n",
    "- [Train the model](#training-models)\n",
    "- [Evaluate the model](#evaluating-models)\n",
    "- [Use the model](#using-models-and-the-numpy-interface)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Survival Datasets \n",
    "\n",
    "PySaRe works with data on the form `(X, T, E)` where:\n",
    "\n",
    "- `X` is a n-dimensional float tensor where each element $i$ in the first dimension are the covariates/features of sample $i$. For PySaRe to work out-of-the-box, `X` must be on this format; however, it is possible to use essentially any list of objects.   \n",
    "- `T` is a one-dimensional float tensor where each element is the recorded time of that sample\n",
    "- `E` is a one-dimensional boolean tensor indicating if the recorded time is a recorded event (True) or censoring (False)\n",
    "\n",
    "Datasets are implemented using the class `pysare.data.Dataset`, and there are a few pre-defined datasets available in PySaRe. Below a simulated Weibull dataset is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysare\n",
    "dataset =  pysare.data.datasets.WeibullUniformParameters(1000)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By printing the dataset it can be seen that the dataset contains 1000 individuals, out of which 570 are censored, and that the shape of the features (shape of each element in the first dimension of `X`) is 2.\n",
    "\n",
    "`X`, `T`, and `E` are available as attributes in the dataset, and we can use this to illustrate how to create a dataset from data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pysare.data.Dataset(X=dataset.X, T=dataset.T, E=dataset.E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finalize this section by partitioning the data into a training, validation, and test part and define the corresponding data loaders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set, validation_set, test_set = dataset.split((0.7,.15))\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "training_loader = DataLoader(training_set, batch_size=128, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=128, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=128, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models  \n",
    "\n",
    "PySaRe models are defined much like how conventional PyTorch Modules are defined, by simply subclassing the desired model type and defining an appropriate forward method.\n",
    "\n",
    "However, in many cases, a multilayer perceptron network is sufficient and therefore implementations of these are readily available using the class method `MLP_implementation`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pysare.models.mixture.GaussianMixture.MLP_implementation(\n",
    "    num_components=5,       # Number of components in the mixture\n",
    "    input_size=2,           # Size of an element of X\n",
    "    hidden_sizes=(32, 32))  # Hidden layer sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom network is not much more difficult and mainly consists of defining a forward function with the correct output size. To determine the correct output size, the method `forward_output_size` can be used. Below the MLP model above is defined using standard PyTorch modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Model(pysare.models.mixture.GaussianMixture):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__() # Most models would also require some model specific \n",
    "                           # parameters to be passe to the base class\n",
    "                           \n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(32,\n",
    "                            # The output from the last layer is determined\n",
    "                            # using the forward_output_size method\n",
    "                            self.forward_output_size(num_components=5)))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the modeling section. For more information se https://pytorch.org/tutorials/ for more PySaRe examples and https://pytorch.org/tutorials/ for more general networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Models \n",
    "\n",
    "In this section we will train the model using maximum likelihood training using the `Trainer` class from `pysare.training`. This trainer support most of the standard ways of training neural networks; however, if a more advanced training is desired, since PySaRe is implemented using PyTorch, most tools and ways of training PyTorch models can be used.\n",
    "\n",
    "Below a PyTorch optimizer is defined in the conventional way, which is then used to define a trainer. The training is run using the data loaders defined above, and finally the training history is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "trainer = pysare.training.Trainer(model=model,\n",
    "                                  optimizer=optimizer)\n",
    "\n",
    "trainer.train(num_max_epochs=200,\n",
    "              training_loader=training_loader,\n",
    "              validation_loader=validation_loader,\n",
    "              early_stopping_patience=50)\n",
    "\n",
    "trainer.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model can provide the log-likelihood, calculating the log-likelihood is straight forward, which can be used to design a custom training procedure. Below, a single step is taken based on the mean log-likelihood of the full training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset gradients\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Get the data from the training set\n",
    "X, T, E = training_set[:]\n",
    "\n",
    "# Calculate the log-likelihood\n",
    "log_likelihood = model.log_likelihood(X, T, E)\n",
    "print(\"First 10 log-likelihoods: \\n\", log_likelihood[:10])\n",
    "\n",
    "# Calculate the mean\n",
    "loss = log_likelihood.mean()\n",
    "print(\"Mean log-likelihood: \\n\", loss)\n",
    "\n",
    "# Calculate gradients\n",
    "loss.mean().backward()\n",
    "\n",
    "# Take the step\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Models\n",
    "\n",
    "To evaluate models, common evaluation metrics are available in the `pysare.evaluation` module. Below, the trained model is evaluated using the Brier score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score = pysare.evaluation.brier_score(\n",
    "    model=model,        # The model\n",
    "    data=test_loader,   # Data on which to evaluate, in this case a loader\n",
    "    times=100,          # Number of points where the score is evaluated\n",
    "    integrated=True,    # Include integrated brier score\n",
    "    plot=True)          # Plot the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the brier score compensates for the censoring distribution; this makes the brier score noisy when the survival function of the censoring distribution is small, therefore censoring distribution is also plotted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Models and the NumPy Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model can be used as is, however, this would require the inputs to be tensors of correct `dtype`, `device` and `shape`. For this reason, PySaRe models contain a NumPy interface that allow using Numpy arrays as inputs. \n",
    "\n",
    "When setting up the interface, the correct type of tensors to use as inputs to the model must be determined. This can be done automatically by the interface, but it is recommended to pass a data loader when setting up the interface. By passing a data loader, its batch size will also be stored and used as maximal batch size for all calls to the underlying PyTorch model.\n",
    "\n",
    "When using the NumPy interface, the features `X`, prediction times `T`, and indicators `E`, should be an `array_like` that after applying `numpy.array` have the following shapes:\n",
    "\n",
    "- `X` should be an array with at least two dimensions and shape `(N, ...)`, where each of the `N` elements in the first dimension are the covariates/features from a specific individual.\n",
    "- `T` should be a float array of shape `(N, 1)`, `(1, M)`, or `(M,)`\n",
    "- `E` should be a boolean array of same shape as `T`.\n",
    "  \n",
    "Depending on the shape of `T` the inputs will be interpreted as follows:\n",
    "  - If the shape is `(N, 1)` the model will be evaluated at `(X[i], T[i], E[i])` for all `i = 0, ..., N`, resulting in a result with shape `(N, 1)`.\n",
    "  - If the shape is `(1, M)` the model will be evaluated at `(X[i], T[j], E[j])` for all `i = 0, ..., N` and `j = 0, ..., M`, resulting in a result of shape `(N, M)`.\n",
    "  - If the shape is `(M,)` it will be interpreted the same as for `(1, M)`. \n",
    "\n",
    "The NumPy interface is accessed through the `to_numpy` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Access the NumPy interface. \n",
    "# The validation_loader is passed to set up the interface\n",
    "np_model = model.to_numpy(validation_loader)\n",
    "\n",
    "# Create one covariate/feature vector for the model\n",
    "scale = 1.5\n",
    "shape = 3.\n",
    "x = (shape, scale)\n",
    "# x is a single covariate/feature vector and must be an element of the first\n",
    "# dimension of X, therefore an extra dimension is added\n",
    "X = (x,)\n",
    "\n",
    "# Times for which to evaluate the model\n",
    "T = np.linspace(0, dataset.T.max(), 100)\n",
    "\n",
    "\n",
    "# For this dataset, the true distribution is known\n",
    "true_model = pysare.models.distributions.Weibull().to_numpy()\n",
    "\n",
    "fig, ax = plt.subplots(3, 1)\n",
    "\n",
    "# Plot the survival function\n",
    "ax[0].plot(T, true_model.survival_function(X, T), label = 'True')\n",
    "ax[0].plot(T, np_model.survival_function(X, T), '--', label = 'Model')\n",
    "ax[0].set_ylabel('Survival function')\n",
    "ax[0].legend()\n",
    "\n",
    "# Plot the density function\n",
    "ax[1].plot(T, true_model.density_function(X, T))\n",
    "ax[1].plot(T, np_model.density_function(X, T), '--')\n",
    "ax[1].set_ylabel('Density function')\n",
    "\n",
    "# Plot the hazard function\n",
    "ax[2].plot(T, true_model.hazard_function(X, T))\n",
    "ax[2].plot(T, np_model.hazard_function(X, T), '--')\n",
    "ax[2].set_ylabel('Hazard function')\n",
    "ax[2].set_xlabel('Time')\n",
    "\n",
    "fig.align_labels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
