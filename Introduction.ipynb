{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This example shows how to train a simple model using PySaRe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pysare\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "A dataset consists of:\n",
    "- A matrix X of features, where the first dimension is the subject-dimension, i.e., each entry in the first dimension corresponds to subject.\n",
    " - A vector T with recorded times for each subject\n",
    " - A vector E with recorded times for each subject E[n] = True for a recorded event and False for a censoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# t_m is the maximal time after which all subjects are censored failed\n",
    "t_m = 3\n",
    "\n",
    "# Simulate 1000 samples from a weibul distribution,\n",
    "X, T, E = pysare.data.datasets.simulate_weibull_uniform_parameters(\n",
    "    1000, output_dataset=False, t_m=t_m)\n",
    "\n",
    "# Create a PySaRe dataset\n",
    "dataset = pysare.data.Dataset(X, T, E)\n",
    "\n",
    "# Slit into training and test set\n",
    "training_set, validation_set = dataset.split(.9)\n",
    "\n",
    "# Define data loaders\n",
    "training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                              shuffle=True,\n",
    "                                              batch_size=1000)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                shuffle=False,\n",
    "                                                batch_size=2000)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model\n",
    "\n",
    "Below, an Energy Based model with a multilayer perceptron (MLP) network with two layers of 100 nodes each, intervals is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t_m is the maximal time after which all subjects are censored failed\n",
    "t_m = 3\n",
    "\n",
    "# Tail ratio used in the integration\n",
    "tail_ratio = 1.2\n",
    "# A monte carlo integration scheme with 20 samples is used for estimating gradients\n",
    "train_integrator = pysare.models.energy_based.integrators.MonteCarlo(20)\n",
    "# The trapezoidal rule on a uniform grid of 20 points is used for evaluation\n",
    "eval_integrator = pysare.models.energy_based.integrators.UniformTrapezoidal(20)\n",
    "# Define model\n",
    "model = pysare.models.energy_based.EBM.MLP_implementation(\n",
    "    t_m, tail_ratio, train_integrator, eval_integrator,\n",
    "    num_features=2, layers=[100, 100])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A model can also be defined using the conventional pytorch style.\n",
    "\n",
    "This is done by specifiying the forward method. It is important that the input and output dimensions are correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_EBM(pysare.models.energy_based.EBM):\n",
    "    def __init__(self):\n",
    "        # t_m and Integrators are passed to super class\n",
    "        super(my_EBM, self).__init__(t_m=t_m, tail_ratio=tail_ratio,\n",
    "                                     train_integrator=train_integrator, eval_integrator=eval_integrator)\n",
    "\n",
    "        # Define network using conventional pytorch\n",
    "        layerlist = []\n",
    "\n",
    "        # first layer\n",
    "        # The input size of the first layer is the number of features plus one\n",
    "        # since time is also an input to the network\n",
    "        input_size = 2+1\n",
    "        layerlist.append(torch.nn.Linear(input_size, 100))\n",
    "        layerlist.append(torch.nn.ReLU())\n",
    "\n",
    "        # Last layer has a single output\n",
    "        layerlist.append(torch.nn.Linear(100, 1))\n",
    "\n",
    "        self.layers = torch.nn.Sequential(*layerlist)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # The first column in X is the time\n",
    "        logits = self.layers(X)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "model = my_EBM()\n",
    "\n",
    "# A torch optimizer is chosen to train the model\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2, weight_decay=0.00)\n",
    "\n",
    "# Path to where the best model is stored during training, when basic_training\n",
    "# finish it will automatically load the best model. Set to None to disable\n",
    "best_model_checkpoint_path = \"intro_best_model\"\n",
    "\n",
    "# Basic training trains the netork using negative log-likelihood as loss function\n",
    "training_log = pysare.training.basic_training(\n",
    "    model,\n",
    "    training_loader,\n",
    "    optimizer,\n",
    "    num_epochs=50,\n",
    "    validation_loader=validation_loader,\n",
    "    best_model_checkpoint_path=best_model_checkpoint_path)\n",
    "\n",
    "# Plot the training\n",
    "training_log.plot()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a loop can be defined by the user or using tools like pytorch-lightning\n",
    "Here an example of a user-defined training loop is shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "model = my_EBM()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-2, weight_decay=0.00)\n",
    "\n",
    "for epoch in range(50):\n",
    "    epoch_loss = 0.0\n",
    "    for batch, (X, T, E) in enumerate(training_loader):\n",
    "\n",
    "        # Compute negative log-likelihood\n",
    "        loss = -model._log_likelihood(X, T, E).sum()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    epoch_loss /= training_loader.dataset.X.shape[0]\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    print(f\"Training loss: {epoch_loss:>7f} \\n-------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Model\n",
    "Here it is shown how the trained model can be used by plotting its survival function and lifetime density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a vector with times to evaluate the model on\n",
    "t = np.linspace(0, 3, 100)\n",
    "\n",
    "# We plot the model for  shape parameter\n",
    "k = 2\n",
    "# and scale parameters\n",
    "l = 2\n",
    "\n",
    "# X should have the shape (N, M1,...) where N is the number of subjects\n",
    "# and (Mi,...) is the shape of a singel feature vector.\n",
    "# Our feature vector is\n",
    "x = [k, l]\n",
    "# and therfore\n",
    "X = [x]\n",
    "# i.e., doulbe brackets are used to indicate that it is a single subject.\n",
    "\n",
    "# Calculate the modeleed survival function and lifetime density\n",
    "S = model.survival_probability(X, t)\n",
    "f = model.lifetime_density(X, t)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, num=1, clear=True)\n",
    "ax[0].plot(t, np.exp(-(t/l)**k), label='True')  # True survival function\n",
    "ax[0].plot(t, S, label='Model')\n",
    "ax[0].set_xlabel('Time')\n",
    "ax[0].set_ylabel('Survival funciton')\n",
    "ax[0].legend()\n",
    "ax[1].plot(t, (k/l)*(t/l)**(k-1)*np.exp(-(t/l)**k))  # True lifetime density\n",
    "ax[1].plot(t, f)\n",
    "ax[1].set_xlabel('Time')\n",
    "ax[1].set_ylabel('Lifetime density')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "Evaluation of the model using common evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_index = pysare.evaluation.concordance_index(model, validation_set)\n",
    "BS = pysare.evaluation.brier_score(model, validation_set, num_t=100)\n",
    "IBS = pysare.evaluation.integrated_brier_score(\n",
    "    model, validation_set, num_t=100)\n",
    "BLL = pysare.evaluation.binomial_log_likelihood(\n",
    "    model, validation_set, num_t=100)\n",
    "IBLL = pysare.evaluation.integrated_binomial_log_likelihood(\n",
    "    model, validation_set, num_t=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all in same figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, num=1, clear=True)\n",
    "C_index = pysare.evaluation.concordance_index(\n",
    "    model, validation_set, ax=ax[[0, 3]])\n",
    "ax[0].set(xlabel=None)\n",
    "ax[0].set(ylabel='C-index')\n",
    "BS = pysare.evaluation.brier_score(\n",
    "    model, validation_set, num_t=100, ax=ax[[1, 3]])\n",
    "# IBS = pysare.evaluation.integrated_brier_score(model, validation_set, num_t=100, ax=ax[[1,4]])\n",
    "ax[3].clear()\n",
    "BLL = pysare.evaluation.binomial_log_likelihood(\n",
    "    model, validation_set, num_t=100, ax=ax[[2, 3]])\n",
    "# IBLL = pysare.evaluation.integrated_binomial_log_likelihood(model, validation_set, num_t=100, ax=ax[[2,4]])\n",
    "ax[0].set(ylabel='C-index')\n",
    "ax[1].set(ylabel='BS')\n",
    "ax[2].set(ylabel='BLL')\n",
    "ax[3].set(ylabel='$S_{cens}(t)$')\n",
    "plt.subplots_adjust(hspace=.1, wspace=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
